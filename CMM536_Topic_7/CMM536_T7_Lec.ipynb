{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlosfmorenog/CMM536/blob/master/CMM536_Topic_7/CMM536_T7_Lec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0ODy_96cKCr"
      },
      "source": [
        "# Topic 7 - Neural Networks\n",
        "![So easy!](https://www.dropbox.com/s/u7fvqjkpm64z4a9/nn101.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIePO7bXcKCw"
      },
      "source": [
        "## Aims of the Session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-qzOb77cKCy"
      },
      "source": [
        "* Learn the basics of neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Not so fast!](https://www.dropbox.com/s/885mxca7i87rpo7/ML_maths.jpg?raw=1)"
      ],
      "metadata": {
        "id": "Rl3D7isHZZkE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKVSXO4hcKCz"
      },
      "source": [
        "## Resources for the Lecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iyZjtmFcKC0"
      },
      "source": [
        "### Online Courses\n",
        "\n",
        "* [Deep Learning Specialization by Andrew NG (Coursera)](https://es.coursera.org/specializations/deep-learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZ4wpPipcKC2"
      },
      "source": [
        "## What is a Neural Network (NN)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3h0EJebcKC3"
      },
      "source": [
        "* Supervised learning classification algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-H6gu-lFcKC4"
      },
      "source": [
        "* **INSPIRED** by how the brain works"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1ZvaWHqcKC6"
      },
      "source": [
        "* Its basis traces back to the 1950's"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46KhYWZ5cKC8"
      },
      "source": [
        "* To understand what an NN does, we need to first understand what a `neuron` does!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAchitF2cKC9"
      },
      "source": [
        "* `Neuron`: The most basic component of an NN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9KreqVqcKC9"
      },
      "source": [
        "* Suppose that you have collected information of different house sizes and their price. You can train a neuron to predict the price of new houses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTbGJaLNcKC-"
      },
      "source": [
        "![Fig. 1. Example of a neuron in a size-price prediction scenario](https://www.dropbox.com/s/zw5ttz6ci7r8age/neuron.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nd994hPHcKC-"
      },
      "source": [
        "### How does a neuron get trained?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T85TWi99cKC-"
      },
      "source": [
        "* A neuron is nothing more than a mathematical function!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxV6_kn5cKC_"
      },
      "source": [
        "* If using `linear regression`, you would consider a basic linear function\n",
        "\n",
        "        y = mx + b??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTtr9rWycKDA"
      },
      "source": [
        "* However, neurons tend to be implemented using other mathematical functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMjl9PqLcKDA"
      },
      "source": [
        "* For the house price problem, we will set our neuron to use the `Rectifying Linear Unit` (ReLu) function, which looks something like this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSgn9plTcKDA"
      },
      "source": [
        "![Fig. 2. Example of the ReLU function](https://www.dropbox.com/s/8o8gt3xjksymp8e/relu.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQng6gIgcKDB"
      },
      "source": [
        "* We use functions like this in our NNs because they are `activation functions` (more on that later)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a42oCDMVcKDB"
      },
      "source": [
        "* Therefore, our house pricing prediction model would look something like this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFCQtalKcKDB"
      },
      "source": [
        "![Fig. 3. Fitting a ReLU function tto predict house prices](https://www.dropbox.com/s/r3i8kmwv36lqc5j/housepred.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMpZdjfwcKDB"
      },
      "source": [
        "### Multiple neurons working together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jxav574scKDC"
      },
      "source": [
        "* What would happen if we want to consider more features besides the size?\n",
        "    * Number of rooms, zip code, wealth, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbvHx1ptcKDC"
      },
      "source": [
        "* We can train a neuron for each feature, but to better understand the relations between them we can interconnect more than one input into different neurons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGWX_omwcKDC"
      },
      "source": [
        "![Fig. 4. A neural network to predict price based on different variables](https://www.dropbox.com/s/kc3etty14m0xtdd/houseprednn.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xvy5J7YOcKDD"
      },
      "source": [
        "* Notice that our NN model is composed of three `layers`:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwogaH6McKDD"
      },
      "source": [
        "* `Input`: The features to consider"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXkqOnzZcKDD"
      },
      "source": [
        "* `Hidden unit`: The neurons that will fit the functions to predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJL4yqmBcKDD"
      },
      "source": [
        "* `Output`: The predicted result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gu9lLtbqcKDE"
      },
      "source": [
        "* The key of understanding how NNs work is in the **hidden unit**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcXZnGTJcKDE"
      },
      "source": [
        "* Notice that in this case, **all inputs** are connected to all neurons in the first hidden layer (this is also known as a `fully connected NN`) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xVc1-AVcKDF"
      },
      "source": [
        "* Then, all neurons connect to a single neuron (second hidden layer) which produces the output!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnIUNV0KcKDG"
      },
      "source": [
        "* The art of NNs is to discover the optimal configuration by means of high computational power"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-vLtr7ocKDG"
      },
      "source": [
        "* In fact, NNs could potentially find second order relations between the features!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO91N6wXcKDG"
      },
      "source": [
        "![Fig. 5. Second order relations between features](https://www.dropbox.com/s/ybz9llj5yn6sl92/houseprednn2ndorder.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvXgneRIcKDG"
      },
      "source": [
        "* Notice that in this new example, not all features are connected to all neurons, i.e. this network is `not fully connected` (which is more often the case!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cGR0OHecKDG"
      },
      "source": [
        "### Why is this concept so trendy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3gZibACcKDG"
      },
      "source": [
        "* Lots of available **LABELLED** data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DO15y50KcKDH"
      },
      "source": [
        "![Fig. 6 a. Why NNs are so popular from the data perspective?](https://www.dropbox.com/s/o1wazkneh44v8vj/whynns.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgOPu_RUcKDH"
      },
      "source": [
        "* Lots of computational power:\n",
        "    * You can try different variables, number of hidden layers, activation functions, etc..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaEpzbP8cKDI"
      },
      "source": [
        "![Fig. 6 b. Why NNs are so popular from the computation perspective?](https://www.dropbox.com/s/xt928ukqwy5egi4/whynns2.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef6mGJqjcKDI"
      },
      "source": [
        "## Neural Network Basics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1y_IAFfecKDJ"
      },
      "source": [
        "### Binary Classification with NNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hktnfZKcKDJ"
      },
      "source": [
        "* As we saw in the last example, a single neuron can simulate a linear regression by means of the ReLU function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpPKqtiTcKDM"
      },
      "source": [
        "* Now we are going to see how an NN can produce a **logistic regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9INfM9Y8cKDL"
      },
      "source": [
        "* We will use colour images as input to predict if these contain a cat or don't!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRd6cwoCcKDN"
      },
      "source": [
        "### Logistic Regression using NNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Qyex1lBcKDN"
      },
      "source": [
        "* A logistic regression calculates the probability of an input to be $0$ or $1$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzuzY465cKDN"
      },
      "source": [
        "* A logistic regression requires the following parameters:\n",
        "    * An input feature vector $x \\in \\mathbb{R}^{n_x}$ where $n_x$ is the number of features\n",
        "    * Training labels $y \\in 0,1$\n",
        "    * A **weights** vector $w \\in \\mathbb{R}^{n_x}$\n",
        "    * A **bias** $b \\in \\mathbb{R}$\n",
        "    * The predicted output:\n",
        "        * For instance, $\\hat{y} = \\sigma(w^{T}x + b)$ where $T$ means *transpose*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVeuvhXOcKDO"
      },
      "source": [
        "* Since we need to predict the **probability** of a sample being 0 or 1, we need to set the `activation function` for the predicted output to something more suitable!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGQ563mrcKDO"
      },
      "source": [
        "* We can use `ReLU`, `tanh` or `sigmoid`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fan-CzMucKDP"
      },
      "source": [
        "* In this lecture we will focus on the **sigmoid function**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4AVJgTLcKDP"
      },
      "source": [
        "![Fig. 7. The sigmoid function](https://www.dropbox.com/s/etk4vkxzc54nw7c/sigmoid.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jqm3j0mKcKDP"
      },
      "source": [
        "* Notice that by default, the sigmoid function is bounded between 0 and 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKow_JG8cKDQ"
      },
      "source": [
        "* Some observations of the sigmoid function:\n",
        "    * $z$ substitutes the whole linear function, therefore it controls the weights and the bias **of the output** (i.e. final hidden layer)\n",
        "    * If $z$ is a large positive number, then $\\sigma(z) = 1$\n",
        "    * If $z$ is small or large negative number, then $\\sigma(z) = 0$\n",
        "    * If $z = 0$, then $\\sigma(z) = 0.5$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXjM4KWncKDQ"
      },
      "source": [
        "### Loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ub2sU-McKDQ"
      },
      "source": [
        "* You want to produce a $\\hat{y}$ which is as similar as possible to $y$!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSax4a2ncKDQ"
      },
      "source": [
        "* The loss (or error) function helps you find how far you are from the actual values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ald-JcsHcKDR"
      },
      "source": [
        "* You could use the **mean square error** between both outputs: $L(y,\\hat{y})=(y-\\hat{y})^2$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkyLgNXScKDR"
      },
      "source": [
        "* This is not convenient in NNs because the produced function becomes **non-convex** (we'll see what's this in a minute!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CJgxhmNcKDS"
      },
      "source": [
        "* A more popular function is:\n",
        "    $L(y,\\hat{y})=(y)(log\\hat{y})+(1-y)(log(1-\\hat{y}))$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RM6VXmVRcKDS"
      },
      "source": [
        "* Remember you want the result of the loss function to be **as small as possible**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvQyH6fUcKDT"
      },
      "source": [
        "### Cost function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWwaxUXYcKDU"
      },
      "source": [
        "* Even when you use the sigmoid function, you still need to learn the parameters $w$ and $b$. **Why?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "228Zc4-ZcKDU"
      },
      "source": [
        "* The cost function depends on the loss function\n",
        "    * This means that in order for us to know the best weights/bias of the NN, we need to know how good is the current output $\\hat{y}$ and then this will iteratively improve our existing NN!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLeWZImAcKDU"
      },
      "source": [
        "* The cost function is defined as $J(w,b) = \\frac{1}{m}\\sum_{i=1}^{m}{L(y,\\hat{y}})$ where:\n",
        "    * $m$ is the number of training samples\n",
        "    * $i$ is the $i^{th}$ sample of the training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MclCSFnfcKDV"
      },
      "source": [
        "### Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rggsbkRcKDV"
      },
      "source": [
        "* You want to find the values of $w$ and $b$ that minimise $J(w,b)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuQ6Yv5ncKDV"
      },
      "source": [
        "* If you plot $w$, $b$ and the cost $J(w,b)$ you will obtain something like this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-ffEw1ycKDV"
      },
      "source": [
        "![Fig. 8. The search space for the values of weight/bias that reduce the cost function](https://www.dropbox.com/s/dt73inxq4i5jkjn/wbcost.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdC5cmSxcKDW"
      },
      "source": [
        "* The previous example was **convex**, which means that it only has **one local minima** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsOcSKdTcKDX"
      },
      "source": [
        "* This is preferable (as you wish to have only one optimal weight/bias combination, and that's why you **don't use mean square error for the loss!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WwcRloccKDX"
      },
      "source": [
        "* Remember, mean square error will create a non-convex function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_F8GsEapcKDY"
      },
      "source": [
        "#### So what is gradient descent!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1o4S_7OcKDY"
      },
      "source": [
        "* An iterative algorithm that starts in a random configuration of weight/bias and stats taking steps down (hopefully!) towards convergence (i.e. the combination of weight/bias that outputs the minimum value of the cost function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4ogfWRZcKDZ"
      },
      "source": [
        "![Fig. 9 a. Gradient descent](https://www.dropbox.com/s/lyfzx5oruyez6y3/gd.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkV6qIjIcKDZ"
      },
      "source": [
        "* where:\n",
        "    * $\\alpha$ is the `learning rate`\n",
        "    * $\\frac{dJ(w)}{dw}$ is the update in the weight value (and yes, it is a derivative!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uS9v2HKqcKDa"
      },
      "source": [
        "![Fig. 9 b. Gradient descent explained](https://www.dropbox.com/s/cam9xjqydt6twcf/gd2.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac3phB7acKDa"
      },
      "source": [
        "#### SO WHAT HAPPENED TO THE BIAS?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxV-KmqFcKDa"
      },
      "source": [
        "* You also update it, in fact the real equations are $w:=w-\\alpha\\frac{dJ(w,b)}{dw}$ and $b:=b-\\alpha\\frac{dJ(w,b)}{db}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7FbD9hlcKDb"
      },
      "source": [
        "### Putting all together (or at least trying to)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76XHqeNIcKDb"
      },
      "source": [
        "* With one example $a$ and two features $x_1$ and $x_2$, training an NN looks like this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzE969LHcKDb"
      },
      "source": [
        "![Fig. 10. Recap of how to implement logistic regression with NNs.](https://www.dropbox.com/s/34rpu9kptmm73al/recap.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvOk5Z5AcKDb"
      },
      "source": [
        "* When you calculate $L(a,y)$ and you update the weight/bias, it is referred to as `backpropagation`!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__i-QVPUcKDc"
      },
      "source": [
        "* In contrast, when you compute the input to a neuron from the outputs of its predecessor neurons as a **weighted sum**, this is referred to as `propagation`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qkn__IpQcKDc"
      },
      "source": [
        "* When doing it for all $m$ examples, you need to sum all individual losses!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH7HPp1IcKDc"
      },
      "source": [
        "* Therefore, you need to add all the derivatives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAa2dLZXcKDd"
      },
      "source": [
        "**DO WE ACTUALLY NEED TO CALCULATE DERIVATIVES IN PYTHON?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNN2IiXncKDd"
      },
      "source": [
        "* How many NN models exist?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHEHR1V7cKDd"
      },
      "source": [
        "![Fig. 11. Different NNs.](https://www.dropbox.com/s/dmsz8i5krh6cy8t/nnmodels.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGyqsilQcKDe"
      },
      "source": [
        "# LAB: TRAINING A LOGISTIC REGRESSION NEURAL NETWORK FROM SCRATCH"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "rise": {
      "enable_chalkboard": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8cGR0OHecKDG",
        "jRd6cwoCcKDN",
        "qXjM4KWncKDQ",
        "HvQyH6fUcKDT",
        "MclCSFnfcKDV",
        "_F8GsEapcKDY",
        "Ac3phB7acKDa",
        "e7FbD9hlcKDb"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}