{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlosfmorenog/CMM536/blob/master/CMM536_Topic_7/CMM536_T7_Lec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9Reas4Vkojj"
      },
      "source": [
        "# CMM536 Topic 7 - Neural Networks\n",
        "![So easy!](https://www.dropbox.com/s/u7fvqjkpm64z4a9/nn101.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD9JJG0Rkojl"
      },
      "source": [
        "## Aims of the Session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI4JrX_kkojn"
      },
      "source": [
        "* Learn the basics of neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht03WoUXkojn"
      },
      "source": [
        "![Not so fast!](https://www.dropbox.com/s/885mxca7i87rpo7/ML_maths.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NILngHfkojo"
      },
      "source": [
        "## Resources for the Lecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRThsOt2kojp"
      },
      "source": [
        "### Online Courses\n",
        "\n",
        "* [Deep Learning Specialization by Andrew NG (Coursera)](https://es.coursera.org/specializations/deep-learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCljf0KZkojq"
      },
      "source": [
        "## What is a Neural Network (NN)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ah-71fUSkojr"
      },
      "source": [
        "* Supervised learning classification algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76mOirTCkojt"
      },
      "source": [
        "* **INSPIRED** by how the brain works"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbKJrd_lkoju"
      },
      "source": [
        "* Its basis traces back to the 1950's"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-43xg3jkoju"
      },
      "source": [
        "* To understand what an NN does, we need to first understand what a `neuron` does!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sarHZEBtkojv"
      },
      "source": [
        "* `Neuron`: The most basic component of an NN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agOJsq4Zkojw"
      },
      "source": [
        "* Suppose that you have collected information of different house sizes and their price. You can train a neuron to predict the price of new houses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2UGDv7Hkojw"
      },
      "source": [
        "![Fig. 1. Example of a neuron in a size-price prediction scenario](https://www.dropbox.com/s/zw5ttz6ci7r8age/neuron.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZDPAEGFkojw"
      },
      "source": [
        "### How does a neuron get trained?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhyV5rlGkojx"
      },
      "source": [
        "* A neuron is nothing more than a mathematical function!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSZMSrotkojy"
      },
      "source": [
        "* If using `linear regression`, you would consider a basic linear function\n",
        "\n",
        "        y = mx + b??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FuJd_jykojy"
      },
      "source": [
        "* However, neurons tend to be implemented using other mathematical functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkEAPArXkojy"
      },
      "source": [
        "* For the house price problem, we will set our neuron to use the `Rectifying Linear Unit` (ReLu) function, which looks something like this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6m0GOcvhkojy"
      },
      "source": [
        "![Fig. 2. Example of the ReLU function](https://www.dropbox.com/s/8o8gt3xjksymp8e/relu.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6-WklaBkojy"
      },
      "source": [
        "* We use functions like this in our NNs because they are `activation functions` (more on that later)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LToiz7Nkojz"
      },
      "source": [
        "* Therefore, our house pricing prediction model would look something like this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1Kd0oV2kojz"
      },
      "source": [
        "![Fig. 3. Fitting a ReLU function tto predict house prices](https://www.dropbox.com/s/r3i8kmwv36lqc5j/housepred.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzkU5FzHkojz"
      },
      "source": [
        "### Multiple neurons working together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJHkc5pFkojz"
      },
      "source": [
        "* What would happen if we want to consider more features besides the size?\n",
        "    * Number of rooms, zip code, wealth, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2CMi89nkoj0"
      },
      "source": [
        "* We can train a neuron for each feature, but to better understand the relations between them we can interconnect more than one input into different neurons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfhXh9GHkoj0"
      },
      "source": [
        "![Fig. 4. A neural network to predict price based on different variables](https://www.dropbox.com/s/kc3etty14m0xtdd/houseprednn.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOP6mzINkoj0"
      },
      "source": [
        "* Notice that our NN model is composed of three `layers`:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdYmrcjlkoj0"
      },
      "source": [
        "* `Input`: The features to consider"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbjQ5Kl3koj1"
      },
      "source": [
        "* `Hidden unit`: The neurons that will fit the functions to predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtFKcm9Mkoj1"
      },
      "source": [
        "* `Output`: The predicted result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHyTarHwkoj1"
      },
      "source": [
        "* The key of understanding how NNs work is in the **hidden unit**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSxFwQcUkoj2"
      },
      "source": [
        "* Notice that in this case, **all inputs** are connected to all neurons in the first hidden layer (this is also known as a `fully connected NN`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfWd2tjzkoj2"
      },
      "source": [
        "* Then, all neurons connect to a single neuron (second hidden layer) which produces the output!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwy4eZXkkoj2"
      },
      "source": [
        "* The art of NNs is to discover the optimal configuration by means of high computational power"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxDg4U3zkoj3"
      },
      "source": [
        "* In fact, NNs could potentially find second order relations between the features!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Esn3Sa5Wkoj3"
      },
      "source": [
        "![Fig. 5. Second order relations between features](https://www.dropbox.com/s/ybz9llj5yn6sl92/houseprednn2ndorder.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2aBgMWwkoj3"
      },
      "source": [
        "* Notice that in this new example, not all features are connected to all neurons, i.e. this network is `not fully connected` (which is more often the case!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NT0Wwqzekoj3"
      },
      "source": [
        "### Why is this concept so trendy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LSVVpDckoj4"
      },
      "source": [
        "* Lots of available **LABELLED** data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2TT0-TBkoj4"
      },
      "source": [
        "![Fig. 6 a. Why NNs are so popular from the data perspective?](https://www.dropbox.com/s/o1wazkneh44v8vj/whynns.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1k32y4Hkoj4"
      },
      "source": [
        "* Lots of computational power:\n",
        "    * You can try different variables, number of hidden layers, activation functions, etc..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hpJJ-egkoj5"
      },
      "source": [
        "## Neural Network Basics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAl3Fd5okoj5"
      },
      "source": [
        "### Binary Classification with NNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unwq1RXIkoj5"
      },
      "source": [
        "* As we saw in the last example, a single neuron can simulate a linear regression by means of the ReLU function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiY_4RCmkoj5"
      },
      "source": [
        "* Now we are going to see how an NN can produce a **logistic regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1o06SrkSkoj6"
      },
      "source": [
        "* We will use colour images as input to predict if these contain a cat or don't!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "190KH-mLkoj6"
      },
      "source": [
        "### Logistic Regression using NNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxThnVVVkoj6"
      },
      "source": [
        "* A logistic regression calculates the probability of an input to be $0$ or $1$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0erNC5Ukoj6"
      },
      "source": [
        "* A logistic regression requires the following parameters:\n",
        "    * An input feature vector $x \\in \\mathbb{R}^{n_x}$ where $n_x$ is the number of features\n",
        "    * Training labels $y \\in 0,1$\n",
        "    * A **weights** vector $w \\in \\mathbb{R}^{n_x}$\n",
        "    * A **bias** $b \\in \\mathbb{R}$\n",
        "    * The predicted output:\n",
        "        * For instance, $\\hat{y} = \\sigma(w^{T}x + b)$ where $T$ means *transpose*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZ5UmkB8koj7"
      },
      "source": [
        "* Since we need to predict the **probability** of a sample being 0 or 1, we need to set the `activation function` for the predicted output to something more suitable!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLvgmuSekoj7"
      },
      "source": [
        "* We can use `ReLU`, `tanh` or `sigmoid`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4Axicj3koj7"
      },
      "source": [
        "* In this lecture we will focus on the **sigmoid function**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioSgCevvkoj8"
      },
      "source": [
        "![Fig. 7. The sigmoid function](https://www.dropbox.com/s/etk4vkxzc54nw7c/sigmoid.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBmctG_Rkoj8"
      },
      "source": [
        "* Notice that by default, the sigmoid function is bounded between 0 and 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRzQJlYUkoj8"
      },
      "source": [
        "* Some observations of the sigmoid function:\n",
        "    * $z$ substitutes the whole linear function, therefore it controls the weights and the bias **of the output** (i.e. final hidden layer)\n",
        "    * If $z$ is a large positive number, then $\\sigma(z) = 1$\n",
        "    * If $z$ is small or large negative number, then $\\sigma(z) = 0$\n",
        "    * If $z = 0$, then $\\sigma(z) = 0.5$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXmJxw4kkoj8"
      },
      "source": [
        "### Loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzlJE1hhkoj9"
      },
      "source": [
        "* You want to produce a $\\hat{y}$ which is as similar as possible to $y$!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eu95d_RSkoj9"
      },
      "source": [
        "* The loss (or error) function helps you find how far you are from the actual values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQi9A-DIkoj9"
      },
      "source": [
        "* You could use the **mean square error** between both outputs: $L(y,\\hat{y})=(y-\\hat{y})^2$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9C8BGxPekoj9"
      },
      "source": [
        "* This is not convenient in NNs because the produced function becomes **non-convex** (we'll see what's this in a minute!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we__MHqKkoj-"
      },
      "source": [
        "* A more popular function is:\n",
        "    $L(y,\\hat{y})=(y)(log\\hat{y})+(1-y)(log(1-\\hat{y}))$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JpwzFQMkoj-"
      },
      "source": [
        "* Remember you want the result of the loss function to be **as small as possible**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJF6TlPJkoj-"
      },
      "source": [
        "### Cost function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EZwTkcckoj-"
      },
      "source": [
        "* Even when you use the sigmoid function, you still need to learn the parameters $w$ and $b$. **Why?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuzzzxSvkoj-"
      },
      "source": [
        "* The cost function depends on the loss function\n",
        "    * This means that in order for us to know the best weights/bias of the NN, we need to know how good is the current output $\\hat{y}$ and then this will iteratively improve our existing NN!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKpfr4C_koj_"
      },
      "source": [
        "* The cost function is defined as $J(w,b) = \\frac{1}{m}\\sum_{i=1}^{m}{L(y,\\hat{y}})$ where:\n",
        "    * $m$ is the number of training samples\n",
        "    * $i$ is the $i^{th}$ sample of the training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-Jq1A--kokA"
      },
      "source": [
        "### Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeBMVOjRkokB"
      },
      "source": [
        "* You want to find the values of $w$ and $b$ that minimise $J(w,b)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18XwH05zkokB"
      },
      "source": [
        "* If you plot $w$, $b$ and the cost $J(w,b)$ you will obtain something like this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF_K5jrCkokC"
      },
      "source": [
        "![Fig. 8. The search space for the values of weight/bias that reduce the cost function](https://www.dropbox.com/s/dt73inxq4i5jkjn/wbcost.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zr2glErjkokC"
      },
      "source": [
        "* The previous example was **convex**, which means that it only has **one local minima**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbG-7KpgkokC"
      },
      "source": [
        "* This is preferable (as you wish to have only one optimal weight/bias combination, and that's why you **don't use mean square error for the loss!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiSOSESWkokC"
      },
      "source": [
        "* Remember, mean square error will create a non-convex function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZgFVvVvkokD"
      },
      "source": [
        "#### So what is gradient descent!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Egj8lrflkokD"
      },
      "source": [
        "* An iterative algorithm that starts in a random configuration of weight/bias and stats taking steps down (hopefully!) towards convergence (i.e. the combination of weight/bias that outputs the minimum value of the cost function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eunLWhbKkokD"
      },
      "source": [
        "![Fig. 9 a. Gradient descent](https://www.dropbox.com/s/lyfzx5oruyez6y3/gd.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NV0pnN3kokD"
      },
      "source": [
        "* where:\n",
        "    * $\\alpha$ is the `learning rate`\n",
        "    * $\\frac{dJ(w)}{dw}$ is the update in the weight value (and yes, it is a derivative!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhPvwfEdkokD"
      },
      "source": [
        "![Fig. 9 b. Gradient descent explained](https://www.dropbox.com/s/cam9xjqydt6twcf/gd2.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39207AzpkokD"
      },
      "source": [
        "#### SO WHAT HAPPENED TO THE BIAS?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK4Va6o3kokD"
      },
      "source": [
        "* You also update it, in fact the real equations are $w:=w-\\alpha\\frac{dJ(w,b)}{dw}$ and $b:=b-\\alpha\\frac{dJ(w,b)}{db}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-VgS-kskokE"
      },
      "source": [
        "### Putting all together (or at least trying to)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSVvkPdYkokE"
      },
      "source": [
        "* With one example $a$ and two features $x_1$ and $x_2$, training an NN looks like this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAqprEn7kokE"
      },
      "source": [
        "![Fig. 10. Recap of how to implement logistic regression with NNs.](https://www.dropbox.com/s/34rpu9kptmm73al/recap.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twS3uWi-kokE"
      },
      "source": [
        "* When you calculate $L(a,y)$ and you update the weight/bias, it is referred to as `backpropagation`!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LwuVCCakokF"
      },
      "source": [
        "* In contrast, when you compute the input to a neuron from the outputs of its predecessor neurons as a **weighted sum**, this is referred to as `propagation`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGjAg08vkokF"
      },
      "source": [
        "* When doing it for all $m$ examples, you need to sum all individual losses!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVL1WKk_kokF"
      },
      "source": [
        "* Therefore, you need to add all the derivatives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keD8PxU2kokF"
      },
      "source": [
        "**DO WE ACTUALLY NEED TO CALCULATE DERIVATIVES IN PYTHON?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1aiaMPtkokG"
      },
      "source": [
        "* How many NN models exist?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJh6rDRLkokG"
      },
      "source": [
        "![Fig. 11. Different NNs.](https://www.dropbox.com/s/dmsz8i5krh6cy8t/nnmodels.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwNTVQ2ukokH"
      },
      "source": [
        "![Fig. 12. Read the math!](https://www.dropbox.com/s/rbivf881hnao64h/readthemath.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjuB9cQ-kokH"
      },
      "source": [
        "# LAB: TRAINING A LOGISTIC REGRESSION NEURAL NETWORK FROM SCRATCH"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "rise": {
      "enable_chalkboard": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}