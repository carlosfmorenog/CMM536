{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMM536 Topic 4 Laboratory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this activity you will use a database of **\"features extracted\"** to perform different types of classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the necessary packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `sklearn` and `scipy` for this activity. If you don't have these packages already, run the cell bellow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sklearn\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `IRIS` database, which contains 150 samples of the sepal and petal lengths and widths from 3 different iris flower species: Setosa, Versicolor and Virginica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Fig 1: Iris dataset.](https://www.dropbox.com/s/kqgsr9tmjdgou4g/iris.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is already available in Python by importing the `sklearn.datasets` function and using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load iris dataset\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Notice that we will **NOT** work with the actual images, but rather with the numerical information extracted from some real flower samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, this dataset was presented in 1936 by a statistician called Ronald Fisher, and thus we don't even have access to the original images to extract some proper feature from!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The dataset is contained on a **dictionary-like** structure referred to as **sklearn.utils.Bunch**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "type(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If you print this dataset, you will see a lot of things contained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Therefore, we need to extract each index of this dictionary into a different variables to understand and analyse them separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First, we will import the `features` into a variable called `data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data = iris['data']\n",
    "print(data)\n",
    "print(type(data))\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The data is stored in a `numpy array` of 150 rows and 4 columns, each corresponding to the  measurements of a flower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then, we will import the `headers` of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "header = iris['feature_names']\n",
    "print(header, type(header))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Why do you think the features and the header are stored separately?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Afterwards, we will import the **class/target**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "target = iris['target']\n",
    "print(target, type(target), target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The class/target is a `numpy array` which contains the **category** of each flowers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Each sample is labelled as $0$, $1$ or $2$ instead of the iris type since the labels can be better used as numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A separate key called **target_names** contains the name corresponding to each numerical label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "target_names = iris['target_names']\n",
    "print(target_names, type(target_names), target_names.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Why do you think the target and the target names are stored separately?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, We can use `list comprehension` to obtain a list of names instead of numerical labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a variable called \"target_named\" and store the actual categories\n",
    "target_named = [target_names[t] for t in target]\n",
    "print(target_named)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, just in case you are interested, there is an entry called `DESC` containing the description of the dataset (a string):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "iris['DESCR']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code and analyse the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the ground truth data\n",
    "## I don't understand fully what half of this code does!\n",
    "import warnings; \n",
    "warnings.simplefilter('ignore')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# if you check \"header\" you will see that we have 4 features\n",
    "# however, it is only possible to plot in 3D!\n",
    "# Therefore, we choose 3 out of 4 features to plot our data\n",
    "# I have tested beforehand all combinations and this is the one that works best\n",
    "# Still, you can change the numbers to see what you get!\n",
    "\n",
    "x_axis = 3 # 3 is petal width\n",
    "y_axis = 0 # 0 is sepal length\n",
    "z_axis = 2 # 2 is petal length\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(1, figsize=(4, 3))\n",
    "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
    "for name, label in [('Setosa', 0),('Versicolor', 1),('Virginica', 2)]:\n",
    "    ax.text3D(data[target == label, x_axis].mean(),\n",
    "              data[target == label, y_axis].mean(),\n",
    "              data[target == label, z_axis].mean() + 2, name,\n",
    "              horizontalalignment='center',\n",
    "              bbox=dict(alpha=.2, edgecolor='w', facecolor='w'))\n",
    "ax.scatter(data[:, 3],data[:, 0],data[:, 2],c=target.astype(np.float),edgecolor='k')\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "ax.set_xlabel(header[x_axis])\n",
    "ax.set_ylabel(header[y_axis])\n",
    "ax.set_zlabel(header[z_axis])\n",
    "ax.set_title('Original Iris Dataset')\n",
    "ax.dist = 12\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the features of the Setosas are notably distinct to the other two types. Therefore, we would expect that a machine learning algorithm is at least capable of identifying all Setosas correctly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering only the features and ignoring the target, we will use an algorithm called **K-means** to **cluster** the 150 iris samples into $k=3$ clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **NOTE:** When testing unsupervised learning, there is no need to split our dataset into training/validation/testing as we are ignoring our labels, therefore the expected output is compared against the actual labels of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this algorithm, we can import the function `Kmeans` from the `sklearn` module and execute it using the following input parameters (the explanation of the parameters can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clustering data using K-means\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=3, init='k-means++', n_init=1, max_iter=300, \n",
    "                tol=0.0001, random_state = 0,\n",
    "                algorithm='auto').fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then store the result of the training in a variable called `kmeans`, which contains the information of how these features can be grouped (i.e. clustered)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can visualise the resulting target by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the target learned by the unsupervised classification algorithm\n",
    "target_kmeans = kmeans.labels_\n",
    "print(target_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar code as the one used to visualise the original dataset can be used to plot the clustered data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this time we will not use the target names, as clustering algorithms can only find a pattern in the data and separate it into groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the results of K-means clustering\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(1, figsize=(4, 3))\n",
    "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
    "for name, label in [('0', 0),('1', 1),('2', 2)]:\n",
    "    ax.text3D(data[target_kmeans == label, x_axis].mean(),\n",
    "              data[target_kmeans == label, y_axis].mean(),\n",
    "              data[target_kmeans == label, z_axis].mean() + 3, name,\n",
    "              horizontalalignment='center',\n",
    "              bbox=dict(alpha=.2, edgecolor='w', facecolor='w'))\n",
    "ax.scatter(data[:, 3],data[:, 0],data[:, 2],c=target_kmeans.astype(np.float),edgecolor='k')\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "ax.set_xlabel(header[x_axis])\n",
    "ax.set_ylabel(header[y_axis])\n",
    "ax.set_zlabel(header[z_axis])\n",
    "ax.set_title('K-means (n_cluster=3)')\n",
    "ax.dist = 12\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we visualy copare both graphs, it appears that the clustering algorithm has clustered setosas as 0, versicolors as 2 and virginicas as 1, however in the original dataset versicolors and virgincas label numbers are opposite!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to compare the similarity between the target obtained using the `K-means` algorithm with respect to the `ground truth`, first we need to exchanged labels \"1\" and \"2\" in `target_kmeans`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exchange labels 1 and 2 in the target obtained using K-means\n",
    "for i, label in enumerate(target_kmeans):\n",
    "    if label==0:\n",
    "        pass\n",
    "    elif label==1:\n",
    "        target_kmeans[i]=2\n",
    "    elif label==2:\n",
    "        target_kmeans[i]=1\n",
    "print(target_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare the two lists by simply using the comparison operation sign and printing the outcome (we will also save it in a variable called `comparison` for future use):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comparison of targets\n",
    "comparison = target_kmeans == target\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To know the **accuracy** of clustering each iris category, we can count and print in groups of 50 the comparisons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the accuracy of clustering each iris category\n",
    "print('Setosa', comparison[:50],'Accuracy', sum(comparison[:50])/len(comparison[:50]))\n",
    "print('Versicolor', comparison[50:100],'Accuracy', sum(comparison[50:100])/len(comparison[50:100]))\n",
    "print('Virginica', comparison[100:],'Accuracy', sum(comparison[100:])/len(comparison[100:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the Setosa type was easier to cluster compared to the other two types, whereas the Virginicas are harder to cluster!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned previously, in unsupervised machine learning you can use your entire dataset to build the model, as the output can be compared with the ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do the same with a supervised machine learning model, you run the risk of **overfitting/overgeneralising** your model and obtaining something that just works for your samples, but not for new ones!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the first thing we need to do is split the dataset in to `training` and `testing` data. For this activity, you are required to reserve **80% of the data for training** and **20% of the data for testing**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can either create your own code to do this division or use a function available elsewhere, just **make sure that samples of the three classes of iris flowers are present on both training and testing data!**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use the following cell to split your dataset into training and testing\n",
    "## Hint: Explore the train_test_split() function in sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you modify your code to ensure that in both datasets there is a balanced number of samples (i.e. 40/40/40 on `training` and 10/10/10 on `testing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use the following cell to split your dataset into training and testing guaranteeing balance\n",
    "## Hint: Investigate about stratification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a SVM classifier using the training data and testing it in the test data. To do so, you need to import `SCV` from the `sklearn.svm` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will train a SVM model with a linear kernel (more information about what this means can be found [here](https://towardsdatascience.com/support-vector-machine-python-example-d67d9b63f1c8)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to **fit** our model and store the result in a variable called `clf`. **FOR VISUALISATION PURPOSES, WE WILL ONLY TRAIN USING THE FIRST TWO FEATURES OF THE DATASET**, which are `sepal length` and `sepal width`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainng the classifier only using the first two columns of the training data\n",
    "clf = model.fit(X_train[:,0:2],y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualise how our model will be capable to classify test samples into the different categories, let's plot the **decision boundary surface of our model**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, we need to define two functions: \n",
    "* `make_meshgrid`: This function will create a mesh object for the minimum and maximum objects.\n",
    "* `plot_contours`: This function will learn how to extract the contours of `clf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_meshgrid(x, y, h=.02):\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "def plot_contours(ax, clf, xx, yy, **params):\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a plot with the mesh grid and the contours using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "title = ('Decision surface of linear SVC')\n",
    "# To create the mesh grid we use the first and second column\n",
    "xx, yy = make_meshgrid(X_train[:, 0],X_train[:, 1])\n",
    "plot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "ax.scatter(X_train[:, 0],X_train[:, 1], c=y_train, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
    "\n",
    "ax.set_xlabel('Sepal Length')\n",
    "ax.set_ylabel('Sepal Width')\n",
    "\n",
    "ax.set_xticks(())\n",
    "ax.set_yticks(())\n",
    "ax.set_title(title)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test our model on the `test` data, we will **predict** it. **Keep in mind that since we trained with only two features, we should also test with the same two features as well**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = model.predict(X_test[:,0:2])\n",
    "print(y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compare the **model's accuracy** of the obtained labels in `y_predicted` against the ground truth in `y_test`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print model's accuracy\n",
    "comparison = y_predicted == y_test\n",
    "print('Accuracy', sum(comparison)/len(comparison))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you print the accuracy for each class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use this cell to print the accuracy for each class\n",
    "## Hint: Use the numpy.where() function to find where does a vector have a certain value i.e. 0/1/2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Try to use different combinations of training/testing split, features used in the model, parameter, etc. Does the accuracy of your model improve? Is it better or worse compared to **Kmeans**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check out how different kernel types affect the performance of the SVM classifier [here](https://scikit-learn.org/0.18/auto_examples/svm/plot_iris.html). Would you consider using a different kernel for this classifier?"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
