{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlosfmorenog/CMM536/blob/master/CMM536_Topic_8/CMM536_T8_Lab_Solved.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SSCxZ_rnDMU"
      },
      "source": [
        "# Topic 8 Lab Solved"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAByEdJgnDMZ"
      },
      "source": [
        "Now that you know the basis of a CNN, you will run one of them in very few lines of code! To do so, we will use `Keras` with a `Tensorflow` backend, along with the very popular `mnist` dataset of handwritten numbers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfNlq1WknDMc"
      },
      "source": [
        "First, install the necessary packages if you don't have them already:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9grvxB__nDMf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "537f2bfe-2bb5-4c87-caea-55ef5082aae0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.5)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.10.0.2)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.13.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.0.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (13.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.44.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.24.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "# 0. Installing the necesssary packages\n",
        "!pip install keras\n",
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VUSmyOSnDMk"
      },
      "source": [
        "Then we will ensure that `Keras` uses `Tensorflow` as backend. Notice that if you prefer to use another backend such as `Theano`, you simply need to change the name in the second line of the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dA00Jy9CnDMm"
      },
      "outputs": [],
      "source": [
        "# 1. Ensure you are using Theano backend\n",
        "import os\n",
        "os.environ['KERAS_BACKEND'] = 'tensorflow'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqaLJp47nDMn"
      },
      "source": [
        "Now you will import the necessary packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nkZUAk8NnDMo"
      },
      "outputs": [],
      "source": [
        "# 2. Import libraries and modules\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOcoU4RynDMq"
      },
      "source": [
        "Then, we will set a **random seed** to be able to repeat the results and get the same results every time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "i0eUxbH6nDMr"
      },
      "outputs": [],
      "source": [
        "# 3. Set random seed (for reproducibility)\n",
        "np.random.seed(123)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-xSJAVJnDMs"
      },
      "source": [
        "With the following cell you will download the data from the `mnist` dataset. Notice that the data comes already partitioned in test and training sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "04S0QwfnnDMt"
      },
      "outputs": [],
      "source": [
        "# 4. Load pre-shuffled MNIST data into train and test sets\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9zkF8UMnDMu"
      },
      "source": [
        "Let's check the shape of the things obtained:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CDm-Hn2YnDMv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "778ebaa1-1f3e-4b9f-ef28-287ba60d2e1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OCbcXrwnDMw"
      },
      "source": [
        "Notice that we have 60'000 samples for training and 10'000 for testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBkxkbCmnDMw"
      },
      "source": [
        "Now we will preprocess the data to be used by the classifier. To be able to use Keras, we need to do the following:\n",
        "1. **Reshape** the data into **four** dimensions i.e. the training set will be of shape (60000,28,28,1) and the test set of (10000,28,28). This is useful since the network needs an input shape of (1,28,28) **for each of the samples**.\n",
        "2. Convert the format of the input into `float32` (apparently the CNN works better with it).\n",
        "3. **Normalise** i.e. divide all values by 255."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tj9YxpYfnDMx"
      },
      "outputs": [],
      "source": [
        "# 5. Preprocess input data\n",
        "# Reshape into four dimensions.\n",
        "X_train_reshape = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
        "X_test_reshape = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
        "# Convert to float 32\n",
        "X_train_reshape = X_train_reshape.astype('float32')\n",
        "X_test_reshape = X_test_reshape.astype('float32')\n",
        "# normalise\n",
        "X_train_reshape /= 255 \n",
        "X_test_reshape /= 255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSOyV4UbnDM0"
      },
      "source": [
        "CNNs also like their target to be categorical, i.e. instead of the target being values from 0 to 9, each target value will be a vector indicating which is the class according to the position. Run the following cell to see what I mean..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "v7QcNnlfnDM1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "84db22fd-ac9e-4891-9060-2fbe396453b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f6f08ef8e50>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# 6. Preprocess class labels\n",
        "Y_train_categorical = np_utils.to_categorical(Y_train, 10)\n",
        "Y_test_categorical = np_utils.to_categorical(Y_test, 10)\n",
        "# Show a sample target entry. You will see that this sample corresponds to a 5 as\n",
        "# there is a five in the 0th position (remember that python starts in 0)\n",
        "print(Y_train_categorical[0])\n",
        "plt.imshow(X_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bgx8rjqdnDM2"
      },
      "source": [
        "Now it's time to train the model. We will define a **sequential** CNN with two convolutional layers, a **max pooling** of size $2 \\times 2$ and a **dropout** of $0.25$. Then, we will add a **flatten** layer, add a **densely connected** layer with a **ReLu** activation, afterwards add another **dropout** of $0.5$, and finally add a densely connected layer to the output with a **softmax** activation function. This configuration is not strict and you can find many different examples, such as [this other one](https://towardsdatascience.com/image-classification-in-10-minutes-with-mnist-dataset-54c35b77a38d)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tYtZp2H6nDM3"
      },
      "outputs": [],
      "source": [
        "# 7. Define model architecture\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
        "\n",
        "model = Sequential()\n",
        " \n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28,28,1)))\n",
        "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation='softmax'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To be congruent with what we saw on the lecture, let's see the model summary. Notice that before the dropout layer we had 589k features! But afterwards only 1290 are used."
      ],
      "metadata": {
        "id": "WcubYnCFSQS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Print a summary of the model\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "CMn9aBgbSQpr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4fcc8e6-7fe9-46be-ad8d-5b146786891a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 24, 24, 32)        9248      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 12, 12, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 12, 12, 32)        0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 4608)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               589952    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 600,810\n",
            "Trainable params: 600,810\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irGWXAn0nDM4"
      },
      "source": [
        "After creating the model architecture, we will compile it. We will use the **ADAM** optimiser to improve the loss obtained by the **categorical cross entropy** method, and then we will request the model to obtain the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rPRq-xp8nDM5"
      },
      "outputs": [],
      "source": [
        "# 9. Compile model\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVlqDQ1PnDM5"
      },
      "source": [
        "Once compiled, we will fit this model using our training data. If your computer is slow, I recommend you **NOT** to use the entire training dataset. This can be done by reducing the number `n` to something smaller than 60'000. Also, you can reduce the number of **epochs**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ta32g1OAnDM6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92951960-dae8-4636-aa17-6ab264f80463"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "188/188 [==============================] - 10s 54ms/step - loss: 0.0422 - accuracy: 0.9868\n",
            "Epoch 2/5\n",
            "188/188 [==============================] - 10s 55ms/step - loss: 0.0406 - accuracy: 0.9873\n",
            "Epoch 3/5\n",
            "188/188 [==============================] - 10s 55ms/step - loss: 0.0368 - accuracy: 0.9873\n",
            "Epoch 4/5\n",
            "188/188 [==============================] - 11s 56ms/step - loss: 0.0353 - accuracy: 0.9882\n",
            "Epoch 5/5\n",
            "188/188 [==============================] - 10s 55ms/step - loss: 0.0421 - accuracy: 0.9847\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6f04751ad0>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# 10. Fit model on training data\n",
        "n=6000 # In google colab, don't worry if you use all the data!\n",
        "\n",
        "model.fit(X_train_reshape[:n], Y_train_categorical[:n], \n",
        "          batch_size=32, epochs=5, verbose=1) # verbose = 1 lets you see the training log for each iteration, higher values just gives you a summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBBg9ykGnDM8"
      },
      "source": [
        "We can try to add more `n` and epochs to see how much `training accuracy` we are capable to achieve. Also, keep in mind that if the `loss` is not going down, then the model is not learning! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmtWTbg1nDM8"
      },
      "source": [
        "Now we can evaluate your model in the `test` data. We can as well obtain the `loss` and the `accuracy` for this new, unseen data for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qEFP8jRfnDM9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1c1998a-f8f0-4543-b118-e3f4bcc31e44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss:  0.11772487312555313 \n",
            "Acc:  0.9623333215713501\n"
          ]
        }
      ],
      "source": [
        "# 11. Evaluate model on test data\n",
        "loss, accuracy = model.evaluate(X_test_reshape[:n], Y_test_categorical[:n], verbose=0)\n",
        "print('Loss: ', loss,'\\nAcc: ', accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IU40tKiqnDM9"
      },
      "source": [
        "With the following cell you can print the labels that were predicted by the model. Notice that the classes are not categorical anymore!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Z4ThHn12nDM-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d284d652-798f-4140-836e-b1829813ad3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7 2 1 ... 9 1 7]\n"
          ]
        }
      ],
      "source": [
        "# 12. Check the labels that have been predicted\n",
        "predict_x=model.predict(X_test[:n]) \n",
        "classes_x=np.argmax(predict_x,axis=1)\n",
        "print(classes_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkjLgh9ZnDM-"
      },
      "source": [
        "The next cell has a brief code that will help you find the incorrect labels by comparing the labels obtained for the test samples with their ground truth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "f-C-VpWPnDM_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efac6a6f-f57a-4f66-8c23-a238a6b2a987"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampele 8 was classified as 6 when it really was 5\n",
            "Sampele 18 was classified as 8 when it really was 3\n",
            "Sampele 92 was classified as 4 when it really was 9\n",
            "Sampele 151 was classified as 8 when it really was 9\n",
            "Sampele 247 was classified as 6 when it really was 4\n",
            "Sampele 266 was classified as 0 when it really was 8\n",
            "Sampele 290 was classified as 4 when it really was 8\n",
            "Sampele 320 was classified as 8 when it really was 9\n",
            "Sampele 321 was classified as 7 when it really was 2\n",
            "Sampele 406 was classified as 6 when it really was 5\n",
            "Sampele 444 was classified as 8 when it really was 2\n",
            "Sampele 445 was classified as 0 when it really was 6\n",
            "Sampele 495 was classified as 0 when it really was 8\n",
            "Sampele 511 was classified as 8 when it really was 4\n",
            "Sampele 578 was classified as 8 when it really was 3\n",
            "Sampele 582 was classified as 2 when it really was 8\n",
            "Sampele 619 was classified as 8 when it really was 1\n",
            "Sampele 659 was classified as 8 when it really was 2\n",
            "Sampele 684 was classified as 2 when it really was 7\n",
            "Sampele 717 was classified as 6 when it really was 0\n",
            "Sampele 720 was classified as 8 when it really was 5\n",
            "Sampele 726 was classified as 9 when it really was 7\n",
            "Sampele 740 was classified as 9 when it really was 4\n",
            "Sampele 839 was classified as 0 when it really was 8\n",
            "Sampele 844 was classified as 7 when it really was 8\n",
            "Sampele 866 was classified as 8 when it really was 5\n",
            "Sampele 883 was classified as 5 when it really was 3\n",
            "Sampele 924 was classified as 7 when it really was 2\n",
            "Sampele 938 was classified as 5 when it really was 3\n",
            "Sampele 939 was classified as 8 when it really was 2\n",
            "Sampele 950 was classified as 2 when it really was 7\n",
            "Sampele 956 was classified as 2 when it really was 1\n",
            "Sampele 965 was classified as 0 when it really was 6\n",
            "Sampele 982 was classified as 2 when it really was 3\n",
            "Sampele 1014 was classified as 5 when it really was 6\n",
            "Sampele 1032 was classified as 8 when it really was 5\n",
            "Sampele 1039 was classified as 9 when it really was 7\n",
            "Sampele 1062 was classified as 7 when it really was 3\n",
            "Sampele 1112 was classified as 6 when it really was 4\n",
            "Sampele 1119 was classified as 2 when it really was 7\n",
            "Sampele 1181 was classified as 1 when it really was 6\n",
            "Sampele 1182 was classified as 8 when it really was 6\n",
            "Sampele 1206 was classified as 2 when it really was 7\n",
            "Sampele 1226 was classified as 2 when it really was 7\n",
            "Sampele 1232 was classified as 8 when it really was 9\n",
            "Sampele 1247 was classified as 0 when it really was 9\n",
            "Sampele 1260 was classified as 1 when it really was 7\n",
            "Sampele 1283 was classified as 2 when it really was 7\n",
            "Sampele 1290 was classified as 5 when it really was 3\n",
            "Sampele 1299 was classified as 7 when it really was 5\n",
            "Sampele 1319 was classified as 0 when it really was 8\n",
            "Sampele 1326 was classified as 2 when it really was 7\n",
            "Sampele 1364 was classified as 2 when it really was 8\n",
            "Sampele 1393 was classified as 3 when it really was 5\n",
            "Sampele 1425 was classified as 6 when it really was 8\n",
            "Sampele 1522 was classified as 9 when it really was 7\n",
            "Sampele 1527 was classified as 6 when it really was 1\n",
            "Sampele 1530 was classified as 7 when it really was 8\n",
            "Sampele 1549 was classified as 6 when it really was 4\n",
            "Sampele 1554 was classified as 8 when it really was 9\n",
            "Sampele 1611 was classified as 5 when it really was 3\n",
            "Sampele 1621 was classified as 6 when it really was 0\n",
            "Sampele 1681 was classified as 7 when it really was 3\n",
            "Sampele 1691 was classified as 8 when it really was 1\n",
            "Sampele 1709 was classified as 5 when it really was 9\n",
            "Sampele 1717 was classified as 0 when it really was 8\n",
            "Sampele 1721 was classified as 9 when it really was 7\n",
            "Sampele 1722 was classified as 7 when it really was 2\n",
            "Sampele 1730 was classified as 8 when it really was 3\n",
            "Sampele 1737 was classified as 8 when it really was 5\n",
            "Sampele 1754 was classified as 2 when it really was 7\n",
            "Sampele 1773 was classified as 6 when it really was 1\n",
            "Sampele 1790 was classified as 8 when it really was 2\n",
            "Sampele 1883 was classified as 9 when it really was 7\n",
            "Sampele 1901 was classified as 4 when it really was 9\n",
            "Sampele 1955 was classified as 2 when it really was 8\n",
            "Sampele 1984 was classified as 0 when it really was 2\n",
            "Sampele 2016 was classified as 2 when it really was 7\n",
            "Sampele 2035 was classified as 3 when it really was 5\n",
            "Sampele 2037 was classified as 6 when it really was 5\n",
            "Sampele 2040 was classified as 6 when it really was 5\n",
            "Sampele 2043 was classified as 8 when it really was 4\n",
            "Sampele 2044 was classified as 7 when it really was 2\n",
            "Sampele 2070 was classified as 9 when it really was 7\n",
            "Sampele 2098 was classified as 0 when it really was 2\n",
            "Sampele 2105 was classified as 8 when it really was 3\n",
            "Sampele 2109 was classified as 8 when it really was 3\n",
            "Sampele 2118 was classified as 0 when it really was 6\n",
            "Sampele 2125 was classified as 9 when it really was 5\n",
            "Sampele 2130 was classified as 9 when it really was 4\n",
            "Sampele 2135 was classified as 1 when it really was 6\n",
            "Sampele 2168 was classified as 2 when it really was 8\n",
            "Sampele 2182 was classified as 2 when it really was 1\n",
            "Sampele 2185 was classified as 8 when it really was 0\n",
            "Sampele 2186 was classified as 0 when it really was 2\n",
            "Sampele 2189 was classified as 1 when it really was 9\n",
            "Sampele 2224 was classified as 6 when it really was 5\n",
            "Sampele 2237 was classified as 8 when it really was 5\n",
            "Sampele 2266 was classified as 6 when it really was 1\n",
            "Sampele 2272 was classified as 0 when it really was 8\n",
            "Sampele 2280 was classified as 5 when it really was 3\n",
            "Sampele 2293 was classified as 0 when it really was 9\n",
            "Sampele 2369 was classified as 6 when it really was 5\n",
            "Sampele 2387 was classified as 1 when it really was 9\n",
            "Sampele 2395 was classified as 0 when it really was 8\n",
            "Sampele 2406 was classified as 1 when it really was 9\n",
            "Sampele 2408 was classified as 5 when it really was 3\n",
            "Sampele 2414 was classified as 8 when it really was 9\n",
            "Sampele 2422 was classified as 4 when it really was 6\n",
            "Sampele 2447 was classified as 9 when it really was 4\n",
            "Sampele 2454 was classified as 8 when it really was 6\n",
            "Sampele 2462 was classified as 0 when it really was 2\n",
            "Sampele 2473 was classified as 8 when it really was 1\n",
            "Sampele 2488 was classified as 4 when it really was 2\n",
            "Sampele 2560 was classified as 2 when it really was 3\n",
            "Sampele 2573 was classified as 8 when it really was 5\n",
            "Sampele 2598 was classified as 1 when it really was 8\n",
            "Sampele 2607 was classified as 8 when it really was 7\n",
            "Sampele 2631 was classified as 6 when it really was 0\n",
            "Sampele 2654 was classified as 1 when it really was 6\n",
            "Sampele 2705 was classified as 8 when it really was 1\n",
            "Sampele 2760 was classified as 4 when it really was 9\n",
            "Sampele 2770 was classified as 8 when it really was 3\n",
            "Sampele 2780 was classified as 3 when it really was 2\n",
            "Sampele 2896 was classified as 0 when it really was 8\n",
            "Sampele 2921 was classified as 2 when it really was 3\n",
            "Sampele 2925 was classified as 0 when it really was 5\n",
            "Sampele 2927 was classified as 2 when it really was 3\n",
            "Sampele 2930 was classified as 7 when it really was 5\n",
            "Sampele 2939 was classified as 7 when it really was 9\n",
            "Sampele 2953 was classified as 5 when it really was 3\n",
            "Sampele 2995 was classified as 8 when it really was 6\n",
            "Sampele 3005 was classified as 8 when it really was 9\n",
            "Sampele 3060 was classified as 7 when it really was 9\n",
            "Sampele 3073 was classified as 2 when it really was 1\n",
            "Sampele 3117 was classified as 9 when it really was 5\n",
            "Sampele 3206 was classified as 3 when it really was 8\n",
            "Sampele 3316 was classified as 4 when it really was 7\n",
            "Sampele 3333 was classified as 9 when it really was 7\n",
            "Sampele 3376 was classified as 9 when it really was 7\n",
            "Sampele 3384 was classified as 6 when it really was 2\n",
            "Sampele 3503 was classified as 1 when it really was 9\n",
            "Sampele 3506 was classified as 6 when it really was 5\n",
            "Sampele 3520 was classified as 4 when it really was 6\n",
            "Sampele 3534 was classified as 8 when it really was 4\n",
            "Sampele 3558 was classified as 0 when it really was 5\n",
            "Sampele 3565 was classified as 8 when it really was 5\n",
            "Sampele 3573 was classified as 2 when it really was 7\n",
            "Sampele 3597 was classified as 5 when it really was 9\n",
            "Sampele 3598 was classified as 8 when it really was 1\n",
            "Sampele 3599 was classified as 7 when it really was 2\n",
            "Sampele 3629 was classified as 0 when it really was 8\n",
            "Sampele 3662 was classified as 0 when it really was 8\n",
            "Sampele 3726 was classified as 9 when it really was 4\n",
            "Sampele 3742 was classified as 9 when it really was 3\n",
            "Sampele 3767 was classified as 2 when it really was 7\n",
            "Sampele 3780 was classified as 6 when it really was 4\n",
            "Sampele 3782 was classified as 2 when it really was 8\n",
            "Sampele 3801 was classified as 0 when it really was 6\n",
            "Sampele 3806 was classified as 8 when it really was 5\n",
            "Sampele 3808 was classified as 8 when it really was 7\n",
            "Sampele 3811 was classified as 3 when it really was 2\n",
            "Sampele 3818 was classified as 6 when it really was 0\n",
            "Sampele 3838 was classified as 1 when it really was 7\n",
            "Sampele 3848 was classified as 2 when it really was 7\n",
            "Sampele 3850 was classified as 4 when it really was 9\n",
            "Sampele 3853 was classified as 0 when it really was 6\n",
            "Sampele 3855 was classified as 0 when it really was 5\n",
            "Sampele 3869 was classified as 4 when it really was 9\n",
            "Sampele 3893 was classified as 6 when it really was 5\n",
            "Sampele 3906 was classified as 2 when it really was 1\n",
            "Sampele 3926 was classified as 8 when it really was 9\n",
            "Sampele 3941 was classified as 6 when it really was 4\n",
            "Sampele 3943 was classified as 5 when it really was 3\n",
            "Sampele 3984 was classified as 8 when it really was 9\n",
            "Sampele 3994 was classified as 8 when it really was 5\n",
            "Sampele 4007 was classified as 9 when it really was 7\n",
            "Sampele 4017 was classified as 8 when it really was 4\n",
            "Sampele 4075 was classified as 6 when it really was 8\n",
            "Sampele 4078 was classified as 2 when it really was 9\n",
            "Sampele 4163 was classified as 0 when it really was 9\n",
            "Sampele 4176 was classified as 7 when it really was 2\n",
            "Sampele 4199 was classified as 9 when it really was 7\n",
            "Sampele 4205 was classified as 8 when it really was 2\n",
            "Sampele 4224 was classified as 7 when it really was 9\n",
            "Sampele 4238 was classified as 3 when it really was 7\n",
            "Sampele 4248 was classified as 8 when it really was 2\n",
            "Sampele 4256 was classified as 0 when it really was 3\n",
            "Sampele 4265 was classified as 1 when it really was 4\n",
            "Sampele 4289 was classified as 7 when it really was 2\n",
            "Sampele 4294 was classified as 7 when it really was 9\n",
            "Sampele 4306 was classified as 7 when it really was 3\n",
            "Sampele 4315 was classified as 8 when it really was 5\n",
            "Sampele 4360 was classified as 9 when it really was 5\n",
            "Sampele 4425 was classified as 4 when it really was 9\n",
            "Sampele 4433 was classified as 2 when it really was 7\n",
            "Sampele 4435 was classified as 7 when it really was 3\n",
            "Sampele 4443 was classified as 2 when it really was 3\n",
            "Sampele 4500 was classified as 1 when it really was 9\n",
            "Sampele 4536 was classified as 8 when it really was 6\n",
            "Sampele 4548 was classified as 6 when it really was 5\n",
            "Sampele 4575 was classified as 2 when it really was 4\n",
            "Sampele 4578 was classified as 9 when it really was 7\n",
            "Sampele 4615 was classified as 4 when it really was 2\n",
            "Sampele 4639 was classified as 9 when it really was 8\n",
            "Sampele 4724 was classified as 0 when it really was 8\n",
            "Sampele 4740 was classified as 5 when it really was 3\n",
            "Sampele 4751 was classified as 6 when it really was 4\n",
            "Sampele 4761 was classified as 8 when it really was 9\n",
            "Sampele 4785 was classified as 8 when it really was 3\n",
            "Sampele 4807 was classified as 0 when it really was 8\n",
            "Sampele 4808 was classified as 5 when it really was 3\n",
            "Sampele 4828 was classified as 8 when it really was 5\n",
            "Sampele 4837 was classified as 2 when it really was 7\n",
            "Sampele 4886 was classified as 2 when it really was 7\n",
            "Sampele 4890 was classified as 6 when it really was 8\n",
            "Sampele 4956 was classified as 4 when it really was 8\n",
            "Sampele 4990 was classified as 2 when it really was 3\n",
            "Sampele 5067 was classified as 2 when it really was 3\n",
            "Sampele 5140 was classified as 0 when it really was 3\n",
            "Sampele 5159 was classified as 9 when it really was 4\n",
            "Sampele 5246 was classified as 2 when it really was 7\n",
            "Sampele 5288 was classified as 9 when it really was 8\n",
            "Sampele 5331 was classified as 6 when it really was 1\n",
            "Sampele 5600 was classified as 9 when it really was 7\n",
            "Sampele 5642 was classified as 8 when it really was 1\n",
            "Sampele 5649 was classified as 9 when it really was 7\n",
            "Sampele 5654 was classified as 9 when it really was 7\n",
            "Sampele 5709 was classified as 9 when it really was 7\n",
            "Sampele 5714 was classified as 9 when it really was 7\n",
            "Sampele 5734 was classified as 7 when it really was 3\n",
            "Sampele 5801 was classified as 0 when it really was 6\n",
            "Sampele 5858 was classified as 9 when it really was 7\n",
            "Sampele 5887 was classified as 0 when it really was 7\n",
            "Sampele 5888 was classified as 0 when it really was 4\n",
            "Sampele 5891 was classified as 6 when it really was 5\n",
            "Sampele 5955 was classified as 8 when it really was 3\n",
            "Sampele 5973 was classified as 8 when it really was 3\n",
            "Sampele 5981 was classified as 9 when it really was 5\n",
            "Sampele 5985 was classified as 8 when it really was 5\n",
            "Sampele 5997 was classified as 9 when it really was 5\n"
          ]
        }
      ],
      "source": [
        "# 13. Check the label that has been predicted incorrectly\n",
        "incorrect_labels=[]\n",
        "accuracy = 0\n",
        "for i,cla in enumerate(classes_x):\n",
        "  if cla != Y_test[:n][i]:\n",
        "    print(\"Sample \"+str(i)+\" was classified as \"+str(cla)+\" when it really was \"+str(Y_test[:n][i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1oAQn11nDNA"
      },
      "source": [
        "Finally, you can use the following cell to print training or test cells from the dataset and see their actual (and predicted, for the case of test) labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "78oido3SnDNA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "45c2369d-6207-4cb6-f70d-6c251c04483d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground truth label:  2\n",
            "Predicted label:  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# 14. Show a sample from the mnist dataset\n",
        "image_to_show = 72\n",
        "from_group = 'test' # 'train' or 'test'\n",
        "if from_group == 'train':\n",
        "    plt.imshow(X_train[image_to_show])\n",
        "    print('Ground truth label: ',Y_train[image_to_show])\n",
        "else:\n",
        "    plt.imshow(X_test[image_to_show])\n",
        "    print('Ground truth label: ',Y_test[image_to_show])\n",
        "    if len(predict_x)>image_to_show:\n",
        "        print('Predicted label: ',predict_x[image_to_show])"
      ]
    }
  ],
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "CMM536_T8_Lab_Solved.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}